\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath} %This allows me to use the align functionality.
                     %If you find yourself trying to replicate
                     %something you found online, ensure you're
                     %loading the necessary packages!
\usepackage{amsfonts}%Math font
\usepackage{graphicx}%For including graphics
\usepackage{hyperref}%For Hyperlinks
\hypersetup{colorlinks = true,citecolor=black}
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography
\usepackage[margin=0.75in]{geometry}
\usepackage{float}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\noindent \textbf{MA 354: Data Analysis I -- Fall 2021}\\%\\ gives you a new line
\noindent \textbf{Homework 4:}\vspace{1em}\\
\emph{Complete the following opportunities to use what we've talked about in class. 
These questions will be graded for correctness, communication and succinctness. 
Ensure you show your work and explain your logic in a legible and refined submission.}
%Comments -- anything after % is not put into the PDF
\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%  Question 0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item[0.] \textbf{Complete weekly diagnostics.}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%  Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item On its website, Ozempic, a medication for lowering the risk of major cardiovascular 
events (e.g., heart attack, stroke, etc.), states that 
\begin{itemize}
  \item 66\% of people taking 0.5 mg Ozempic
  \item 73\% of people taking 1 mg Ozempic
  \item 40\% of people taking 100 mg Januvia
\end{itemize}
reached an A1C under 7\%, noting higher A1C is indicative of higher risk of heart disease.

\begin{enumerate}
\item Explain why this statement alone isn't enough to conclude whether there is a statistically 
significant difference among the treatments.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstr{"To assess any significant difference we need ot run a t-sample proportion test, but since we only have the information on the proportion of the people seeing beneficial affect in the above question, it is not sufficient information"}
\end{alltt}
\begin{verbatim}
## [1] "To assess any significant difference we need ot run a t-sample proportion test, but since we only have the information on the proportion of the people seeing beneficial affect in the above question, it is not sufficient information"
\end{verbatim}
\end{kframe}
\end{knitrout}
\item The statement on Ozempic's website comes from a phase 3a randomized double-blind study. 
\cite{Ahren17} reports that 409 received Ozempic (0.5 mg), 409 received Ozempic (1 mg), and 407 
received Januvia (100 mg). 
\begin{enumerate}
  \item Determine whether there is sufficient evidence of a difference in rates of attaining an 
  A1C under 7\% across treatments.
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#https://pubmed.ncbi.nlm.nih.gov/28385659/}
\hlcom{#Mean baseline HbA1c was 8Â·1%}
\hlcom{#2)}

\hlcom{### Remember to write about the assumptions of doing t-sample prop.test#####}
\hlcom{# Successful and total cases for 0.5 mg Ozempic}
\hlstd{x1} \hlkwb{=} \hlkwd{round}\hlstd{(}\hlnum{0.66}\hlopt{*}\hlnum{409}\hlstd{)}
\hlstd{n1} \hlkwb{=} \hlnum{409}

\hlcom{# Successful and total cases for 1 mg Ozempic}
\hlstd{x2} \hlkwb{=} \hlkwd{round}\hlstd{(}\hlnum{0.73}\hlopt{*}\hlnum{409}\hlstd{)}
\hlstd{n2} \hlkwb{=} \hlnum{409}

\hlcom{# Successful and total cases for 100 mg Januvia}
\hlstd{x3} \hlkwb{=} \hlkwd{round}\hlstd{(}\hlnum{0.4}\hlopt{*}\hlnum{407}\hlstd{)}
\hlstd{n3} \hlkwb{=} \hlnum{407}

\hlkwd{prop.test}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{c}\hlstd{(x1, x2, x3),} \hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(n1, n2, n3))}
\end{alltt}
\begin{verbatim}
## 
## 	3-sample test for equality of proportions without continuity
## 	correction
## 
## data:  c(x1, x2, x3) out of c(n1, n2, n3)
## X-squared = 102.7, df = 2, p-value < 2.2e-16
## alternative hypothesis: two.sided
## sample estimates:
##    prop 1    prop 2    prop 3 
## 0.6601467 0.7310513 0.4004914
\end{verbatim}
\begin{alltt}
\hlcom{# There is a significant p-value to support a difference ()}


\hlcom{#t sample proportion test}
\hlcom{#DO IT}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Perform a follow-up analysis for comparing treatments. If you were at high risk for 
  cardiovascular events, which medication would you want to take.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pairwise.prop.test}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{c}\hlstd{(x1, x2, x3),} \hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(n1, n2, n3))}
\end{alltt}
\begin{verbatim}
## 
## 	Pairwise comparisons using Pairwise comparison of proportions 
## 
## data:  c(x1, x2, x3) out of c(n1, n2, n3) 
## 
##   1       2      
## 2 0.033   -      
## 3 3.6e-13 < 2e-16
## 
## P value adjustment method: holm
\end{verbatim}
\begin{alltt}
\hlcom{# There is a significant difference between 2 and 3 and 1 and 3 success proportions. So}
\hlcom{# 1 and 2 are better than 3. But if we have alpha = 0.05 then there is also a significant }
\hlcom{# difference between 1 and 2, in which case 2 (1 mg of Ozempic) is better.}
\end{alltt}
\end{kframe}
\end{knitrout}


\end{enumerate}
\end{enumerate}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%  Question 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Is the ANOVA really robust to Normality? Equal sample size? Equal variance?
To assess this we'll check the ability of ANOVA to detect differences in a 
sample and retain the $\alpha=0.05$ across different settings. This homework 
question was motivated by \cite{Blanca17} who published a simulation study 
about ANOVA.\\

\textbf{Remark:} My professor in graduate school always told me that I didn't
have to memorize any results, I could just derive them. The data analysis
analog to this is that if we have any questions about how a model works
under a given condition (or broken assumption) we can just simulate it!
\begin{enumerate}
  \item Plot the Laplace distribution with $m=0$ and $s=2$; the PDF
  of this distribution is cataloged in \texttt{R} as \texttt{dlaplace()} 
  in the rmutil package, which you'll need to install and load. Superimpose
  the graph of the Gaussian distribution with $\mu=0$ and $\sigma=2$. Comment
  on the differences you see and what you think might happen if the data 
  are Laplace distributed instead of the Gaussian distribution.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(rmutil)}
\hlkwd{library}\hlstd{(tidyverse)}

\hlstd{ggdat} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlopt{-}\hlnum{5}\hlstd{,} \hlnum{5}\hlstd{,} \hlkwc{length.out}\hlstd{=}\hlnum{5000}\hlstd{))}\hlopt{%>%}
\hlkwd{mutate}\hlstd{(}\hlkwc{f}\hlstd{=}\hlkwd{dlaplace}\hlstd{(x,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=}\hlnum{2}\hlstd{),}
       \hlkwc{f1}\hlstd{=}\hlkwd{dnorm}\hlstd{(x,} \hlkwc{mean}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{sd}\hlstd{=}\hlnum{2}\hlstd{))}

\hlcom{#set the legend}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=f))}\hlopt{+}
\hlkwd{geom_line}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"blue"}\hlstd{)}\hlopt{+}
\hlkwd{geom_line}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y}\hlstd{=f1),} \hlkwc{color}\hlstd{=}\hlstr{"red"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-4-1} 
\end{knitrout}
  \item Conduct a simulation study using the Laplace distribution. To do so, 
  complete the following 1000 times and report the proportion of times the
  data lead to a rejection of the null hypothesis.\\
  
  The most efficient way to complete this question (including the other parts)
  is to write a function that completes the following. 
  \begin{itemize}
    \item \textbf{Input:} 
    \begin{itemize}
      \item \texttt{rand.n=FALSE} -- a logical object denoting whether the sample
                                     size is random or not. False by default.
                                     See part (f).
      \item \texttt{rand.s=FALSE} -- a logical object denoting whether the dispersion
                                     equal or not not. False by default. See part (g).
      \item \texttt{equal.m=TRUE} -- a logical object denoting whether the location
                                     parameters should be equal (part b) or different 
                                     (part c). TRUE by default. 
      \item \texttt{n=5} -- the desired sample size if not random. Five by default.
    \end{itemize}
    \item \textbf{Loop the following tasks 1000 times:}
    \begin{itemize}
      \item Generate $t=4$ samples of size $n$ drawn independently from the 
        laplace distribution with $m$ and $s$ which can be done using 
        \texttt{rlaplace()} function from the rmutil package \citep{rmutil}. Specify
        $n$, $m$, and $s$ based on the values of the
        logical variables described above.
      \item Perform the ANOVA procedure on these generated data.
      \item Store whether the test resulted in a rejected null hypothesis or not.
    \end{itemize}
    \item \textbf{Return:}
    \begin{itemize}
      \item Your function should return the proportion of the 1000 ANOVA tests
      that resulted in a rejected null hypothesis.
    \end{itemize}
  \end{itemize}
    Comment on the results of this simulation completed in the default case where
  $m_1=m_2=m_3=m_4=0$, $s_1=s_2=s_3=s_4=2$, and $n_1=n_2=n_3=n_4=5$
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(rstatix)}
\hlstd{aovFunc} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{rand.n}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{rand.s}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{equal.m}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{n}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{0}\hlstd{,}
                    \hlkwc{welch}\hlstd{=}\hlnum{FALSE}\hlstd{)\{}
  \hlstd{alpha}\hlkwb{<-}\hlnum{0.05}
  \hlstd{count}\hlkwb{=}\hlnum{0}
  \hlkwa{if}\hlstd{(rand.n)\{}
    \hlstd{n1}\hlkwb{=}\hlkwd{round}\hlstd{(}\hlkwd{runif}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{min}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{max}\hlstd{=}\hlnum{100}\hlstd{),}\hlnum{0}\hlstd{)}
    \hlstd{n2}\hlkwb{=}\hlkwd{round}\hlstd{(}\hlkwd{runif}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{min}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{max}\hlstd{=}\hlnum{100}\hlstd{),}\hlnum{0}\hlstd{)}
    \hlstd{n3}\hlkwb{=}\hlkwd{round}\hlstd{(}\hlkwd{runif}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{min}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{max}\hlstd{=}\hlnum{100}\hlstd{),}\hlnum{0}\hlstd{)}
    \hlstd{n4}\hlkwb{=}\hlkwd{round}\hlstd{(}\hlkwd{runif}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{min}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{max}\hlstd{=}\hlnum{100}\hlstd{),}\hlnum{0}\hlstd{)}
  \hlstd{\}}
  \hlkwa{else}\hlstd{\{}
    \hlstd{n1}\hlkwb{=}\hlstd{n}
    \hlstd{n2}\hlkwb{=}\hlstd{n}
    \hlstd{n3}\hlkwb{=}\hlstd{n}
    \hlstd{n4}\hlkwb{=}\hlstd{n}
  \hlstd{\}}

  \hlkwa{if}\hlstd{(rand.s)\{}
    \hlcom{#DOUBLE CHECK WITH PROFESSOR}
    \hlstd{s1}\hlkwb{=}\hlkwd{rgamma}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{)}
    \hlstd{s2}\hlkwb{=}\hlkwd{rgamma}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{)}
    \hlstd{s3}\hlkwb{=}\hlkwd{rgamma}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{)}
    \hlstd{s4}\hlkwb{=}\hlkwd{rgamma}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{)}
  \hlstd{\}}\hlkwa{else}\hlstd{\{}
    \hlstd{s1}\hlkwb{=}\hlnum{1}
    \hlstd{s2}\hlkwb{=}\hlnum{1}
    \hlstd{s3}\hlkwb{=}\hlnum{1}
    \hlstd{s4}\hlkwb{=}\hlnum{1}
  \hlstd{\}}
  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{loop)\{}
    \hlkwa{if}\hlstd{(equal.m)\{}
      \hlstd{t1}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n1,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s1)}
      \hlstd{label1}\hlkwb{<-}\hlstr{"T1"}

      \hlstd{t2}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n2,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s2)}
      \hlstd{label2}\hlkwb{<-}\hlstr{"T2"}

      \hlstd{t3}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n3,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s3)}
      \hlstd{label3}\hlkwb{<-}\hlstr{"T3"}

      \hlstd{t4}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n4,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s4)}
      \hlstd{label4}\hlkwb{<-}\hlstr{"T4"}
    \hlstd{\}}

    \hlkwa{else}\hlstd{\{}
      \hlstd{t1}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n1,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s1)}
      \hlstd{label1}\hlkwb{<-}\hlstr{"T1"}

      \hlstd{t2}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n2,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s2)}
      \hlstd{label2}\hlkwb{<-}\hlstr{"T2"}

      \hlstd{t3}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n3,} \hlkwc{m}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{s}\hlstd{=s3)}
      \hlstd{label3}\hlkwb{<-}\hlstr{"T3"}

      \hlstd{t4}\hlkwb{<-}\hlkwd{rlaplace}\hlstd{(}\hlkwc{n}\hlstd{=n4,} \hlkwc{m}\hlstd{=}\hlnum{1}\hlstd{,} \hlkwc{s}\hlstd{=s4)}
      \hlstd{label4}\hlkwb{<-}\hlstr{"T4"}
    \hlstd{\}}

    \hlstd{dat}\hlkwb{<-}\hlkwd{data.frame}\hlstd{(}\hlkwc{value}\hlstd{=}\hlkwd{c}\hlstd{(t1, t2, t3, t4),}
                    \hlkwc{group}\hlstd{=}\hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"T1"}\hlstd{,} \hlstr{"T2"}\hlstd{,} \hlstr{"T3"}\hlstd{,} \hlstr{"T4"}\hlstd{),}
                                                         \hlkwc{times}\hlstd{=}\hlkwd{c}\hlstd{(}\hlkwd{length}\hlstd{(t1),}
                                                                 \hlkwd{length}\hlstd{(t2),}
                                                                 \hlkwd{length}\hlstd{(t3),}
                                                                 \hlkwd{length}\hlstd{(t4)))))}

    \hlkwa{if}\hlstd{(}\hlopt{!}\hlstd{welch)\{}
      \hlstd{anova}\hlkwb{<-}\hlkwd{summary}\hlstd{(}\hlkwd{aov}\hlstd{(value}\hlopt{~}\hlstd{group,} \hlkwc{data}\hlstd{=dat))}
      \hlstd{sum_test} \hlkwb{<-} \hlkwd{unlist}\hlstd{((anova))}
      \hlstd{p.value}\hlkwb{<-}\hlstd{sum_test[}\hlstr{"Pr(>F)1"}\hlstd{]}
      \hlcom{#print(p.value) #bugtest}
    \hlstd{\}}\hlkwa{else}\hlstd{\{}
      \hlstd{anova_w}\hlkwb{<-}\hlkwd{welch_anova_test}\hlstd{(value}\hlopt{~}\hlstd{group,} \hlkwc{data}\hlstd{=dat)}
      \hlstd{p.value}\hlkwb{<-}\hlstd{anova_w}\hlopt{$}\hlstd{p}
    \hlstd{\}}

    \hlkwa{if}\hlstd{(p.value}\hlopt{<}\hlnum{0.05}\hlstd{)\{}
     \hlstd{count}\hlkwb{=}\hlstd{count}\hlopt{+}\hlnum{1}
    \hlstd{\}}
  \hlstd{\}}
  \hlstd{count}\hlopt{/}\hlstd{loop}
\hlstd{\}}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.03
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item Repeat the simulation study in (b-d), except with different means; i.e.,
  $m_1=m_2=m_3=0$, and $m_4=1$. Comment on the results of this simulation.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.22
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Repeat the simulation study in (b-c), except with $n=15$. Comment on the results of this simulation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{,} \hlkwc{n}\hlstd{=}\hlnum{15}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.07
\end{verbatim}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{,} \hlkwc{n}\hlstd{=}\hlnum{15}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.46
\end{verbatim}
\end{kframe}
\end{knitrout}
  \item Repeat the simulation study in (b-c), except with $n=50$.
  Comment on the results of this simulation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{,} \hlkwc{n}\hlstd{=}\hlnum{50}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.04
\end{verbatim}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{,} \hlkwc{n}\hlstd{=}\hlnum{50}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Repeat (b-e), except randomly select the sample size for each 
group by selecting $n$ from the uniform(5,100) distribution. This 
will help us assess the robustness of the equal sample size assumption
in the Laplace population distribution case. Comment on the results of 
this simulation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{rand.n}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.07
\end{verbatim}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{rand.n}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.98
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Repeat (b-f), except randomly select the dispersion for each 
  group by selecting $s$ from the gamma(2,1) distribution. This 
  will help us assess the robustness of the equal variance assumption
  in the Laplace population distribution case. Comment on the results of 
  this simulation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{rand.n}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{rand.s}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.02
\end{verbatim}
\begin{alltt}
\hlkwd{aovFunc}\hlstd{(}\hlkwc{equal.m}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{rand.n}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{rand.s}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{loop}\hlstd{=}\hlnum{100}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.61
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Write a loop that conducts this simulation when (1) $s$ is fixed and (2) 
when $s$ is random, for the case where the means are unequal. The loop should be 
with respect to $n$, and should run for $n=5$ to $n=200$.\\

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# results<-c()}
\hlcom{# n<-10:50}
\hlcom{# y1<-c()}
\hlcom{# y2<-c()}
\hlcom{# y3<-c()}
\hlcom{# }
\hlcom{# for(i in 10:50)\{}
\hlcom{#   print(i)}
\hlcom{#   unequal<-aovFunc(equal.m=FALSE, rand.s=TRUE, loop=100, n=i)}
\hlcom{#   y1<-c(y1, unequal)}
\hlcom{#   equal<-aovFunc(equal.m=FALSE, rand.s=FALSE, loop=100, n=i)}
\hlcom{#   y2<-c(y2, equal)}
\hlcom{#   }
\hlcom{#   wUnequal<-aovFunc(equal.m=FALSE, rand.s=TRUE, loop=100, n=i, welch=T)}
\hlcom{#   y3<-c(y3, wUnequal)}
\hlcom{# \}}
\hlcom{# }
\hlcom{# ggdat<-data.frame(x=n, equal.s=y2, unequal.s=y1,}
\hlcom{#                   welch=y3)}
\hlcom{# ggplot(ggdat, aes(x=x))+}
\hlcom{#   geom_line(aes(y=equal.s), color="red")+}
\hlcom{#   geom_line(aes(y=unequal.s), color="blue")+}
\hlcom{#   geom_line(aes(y=welch), color="black")+}
\hlcom{#   labs(title="Everybody hates Welch")}
\end{alltt}
\end{kframe}
\end{knitrout}
\textbf{Note:} This can take some computation time, you'll want to run it and save 
the image as a .pdf so you can load it instead of rerunning the code.
\end{enumerate}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%  Question 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Complete the following parts. This will lead you through the simulation
  of data, fitting regression lines and evaluating the assumptions.
  \begin{enumerate}
  \item Fit a model to the following simulated data. Make observations about
  the model equation and the Pearson correlation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n}\hlkwb{=}\hlnum{500}
\hlstd{x}\hlkwb{<-}\hlkwd{sample}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{0.01}\hlstd{),} \hlkwc{size}\hlstd{=n,} \hlkwc{replace}\hlstd{=T)}
\hlstd{y}\hlkwb{<-}\hlnum{5}\hlopt{*}\hlstd{x} \hlopt{+} \hlnum{3}

\hlstd{ggdat}\hlkwb{<-}\hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y)}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"blue"}\hlstd{,}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-12-1} 
\begin{kframe}\begin{alltt}
  \hlcom{#put pearson on the plot}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Fit a model to the following simulated data, now with added Normal error. Make
  observations about the model equation and the Pearson correlation in relation to (a).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{e}\hlkwb{<-}\hlkwd{rnorm}\hlstd{(}\hlkwc{n}\hlstd{=n,}\hlkwc{mean}\hlstd{=}\hlnum{0}\hlstd{,}\hlkwc{sd}\hlstd{=}\hlnum{3}\hlstd{)}
\hlstd{y2}\hlkwb{<-}\hlnum{5}\hlopt{*}\hlstd{x} \hlopt{+} \hlnum{3} \hlopt{+} \hlstd{e}

\hlstd{ggdat}\hlkwb{<-}\hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y2)}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"blue"}\hlstd{,}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-13-1} 
\begin{kframe}\begin{alltt}
  \hlcom{#put pearson on the plot}
\hlcom{#cor(x, y2, method="pearson")}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item In the model of part (b), evaluate the normality and homogeneity of error terms. Note 
  that we know both of these items to be true since we've taken $\epsilon \sim 
  \textrm{N}(\mu=0,\sigma=3)$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Preparing the functions!}
\hlkwd{library}\hlstd{(patchwork)}
\hlkwd{source}\hlstd{(}\hlstr{"https://cipolli.com/students/code/plotResiduals.R"}\hlstd{)}
\hlkwd{source}\hlstd{(}\hlstr{"https://cipolli.com/students/code/plotInfluence.R"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{<-} \hlkwd{lm}\hlstd{(y2}\hlopt{~}\hlstd{x)}
\hlkwd{plotResiduals}\hlstd{(model1)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-15-1} 
\begin{kframe}\begin{alltt}
\hlcom{#Variance is the same}
\hlcom{#https://uc-r.github.io/assumptions_homogeneity#visualization}
\end{alltt}
\end{kframe}
\end{knitrout}
\item Fit a model to the following simulated data, now with added exponential error.
  Make observations about the model equation and the Pearson correlation in relation 
  to the model of part (b).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{e}\hlkwb{<-}\hlkwd{rexp}\hlstd{(}\hlkwc{n}\hlstd{=n,}\hlkwc{rate} \hlstd{=} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{)}
\hlstd{y3}\hlkwb{<-}\hlnum{5}\hlopt{*}\hlstd{x} \hlopt{+} \hlnum{3} \hlopt{+} \hlstd{e}

\hlstd{ggdat}\hlkwb{<-}\hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y3)}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"blue"}\hlstd{,}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-16-1} 
\begin{kframe}\begin{alltt}
  \hlcom{#put pearson on the plot}
\hlkwd{cor}\hlstd{(x, y3,} \hlkwc{method}\hlstd{=}\hlstr{"pearson"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.9701446
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item In the model of part (d), evaluate the normality and homogeneity of error 
  terms. Note that we know that common variance is true but we've taken $\epsilon \sim 
  \textrm{exp}(\beta=2)$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{<-} \hlkwd{lm}\hlstd{(y3}\hlopt{~}\hlstd{x)}
\hlkwd{plotResiduals}\hlstd{(model2)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-17-1} 
\begin{kframe}\begin{alltt}
\hlcom{#It's a skewed normal dist, but it's homogeneous.}
\end{alltt}
\end{kframe}
\end{knitrout}
\item Fit a model to the following simulated data, now with added Heteroskedastic
normal error. Make observations about the model equation and the Pearson correlation
in relation to the model of part (b).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x4}\hlkwb{<-}\hlstd{x[}\hlkwd{order}\hlstd{(x)]}
\hlstd{e}\hlkwb{<-}\hlkwd{rnorm}\hlstd{(}\hlkwc{n}\hlstd{=n,}\hlkwc{mean}\hlstd{=}\hlnum{0}\hlstd{,}\hlkwc{sd}\hlstd{=}\hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,n}\hlopt{/}\hlnum{2}\hlstd{),}\hlkwd{rep}\hlstd{(}\hlnum{3}\hlstd{,n}\hlopt{/}\hlnum{2}\hlstd{)))}
\hlstd{y4}\hlkwb{<-}\hlnum{5}\hlopt{*}\hlstd{x4} \hlopt{+} \hlnum{3} \hlopt{+} \hlstd{e}

\hlstd{ggdat}\hlkwb{<-}\hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=x4,} \hlkwc{y}\hlstd{=y4)}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"blue"}\hlstd{,}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-18-1} 
\end{knitrout}
  \item In the model of part (f), evaluate the normality and homogeneity of error terms. Note
  that we know that normality of error terms is true, but $\epsilon \sim 
  \textrm{N}(\mu=0,\sigma=1)$ for $x<\widehat{m}$ and $\epsilon \sim 
  \textrm{N}(\mu=0,\sigma=3)$ for $x>\widehat{m}$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model3} \hlkwb{<-} \hlkwd{lm}\hlstd{(y4}\hlopt{~}\hlstd{x4)}
\hlkwd{plotResiduals}\hlstd{(model3)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-19-1} 
\end{knitrout}
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%  Question 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Consider the following simulation.
  \begin{enumerate}
    \item Plot the data simulated below. Assess the linear relationship.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(tidyverse)}
\hlkwd{set.seed}\hlstd{(}\hlnum{7272}\hlstd{)}
\hlstd{n}\hlkwb{<-}\hlnum{50}
\hlstd{ggdat} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{x}\hlstd{=}\hlkwd{sample}\hlstd{(}\hlkwc{x}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{100}\hlstd{,}\hlnum{0.01}\hlstd{),}\hlkwc{size}\hlstd{=n,}\hlkwc{replace}\hlstd{=}\hlnum{TRUE}\hlstd{))} \hlopt{%>%}
  \hlkwd{mutate}\hlstd{(}\hlkwc{y}\hlstd{=}\hlnum{3.5}\hlopt{+}\hlnum{2.1}\hlopt{*}\hlstd{x}\hlopt{+}\hlkwd{rnorm}\hlstd{(}\hlkwc{n}\hlstd{=n,}\hlkwc{mean}\hlstd{=}\hlnum{0}\hlstd{,}\hlkwc{sd}\hlstd{=}\hlnum{5}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Write out the population model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Y=B\{0\}+B\{1\}\{x\} + error}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Fit the model based on the sample data and write out the sample model below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#hat(Y)=hat(B\{0\})+hat(B)\{1\}\{x\}}
\hlstd{four.model}\hlkwb{<-}\hlkwd{lm}\hlstd{(y}\hlopt{~}\hlstd{x,} \hlkwc{data}\hlstd{=ggdat)}
\hlkwd{summary}\hlstd{(four.model)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x, data = ggdat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.2855  -2.9153  -0.0545   2.7938  14.7084 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  5.22286    1.44261    3.62 0.000707 ***
## x            2.06056    0.02345   87.89  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.152 on 48 degrees of freedom
## Multiple R-squared:  0.9938,	Adjusted R-squared:  0.9937 
## F-statistic:  7724 on 1 and 48 DF,  p-value: < 2.2e-16
\end{verbatim}
\begin{alltt}
\hlcom{#hat(B)\{0\}=5.22}
\hlcom{#hat(B)\{1\}\{x\}=2.06}

\hlcom{#hat(Y)=-5.22+2.06x}

\hlcom{#Prediction = 5.22+2.06(x) + (random error)}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Add the regression line to the plot in black.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(ggdat,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=y))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{color}\hlstd{=}\hlstr{"black"}\hlstd{,}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-23-1} 
\end{knitrout}
    \item Interpret the $R^2$ of the model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Adjusted R-squared:  0.9937 }
\hlcom{#99% of the variance can be explained by the model we built.}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Interpret the overall $F$ test of the model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Look at p-value}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Interpret the coefficients of the model; are they what you would expect?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Yeah, that's easy.}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item Now, let's add a bad datapoint to the data.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ggdat} \hlkwb{<-} \hlkwd{rbind}\hlstd{(ggdat,}     \hlcom{# original data}
               \hlkwd{c}\hlstd{(}\hlnum{100}\hlstd{,}\hlnum{25}\hlstd{))} \hlcom{# bad observation}
\end{alltt}
\end{kframe}
\end{knitrout}
  \begin{enumerate}
    \item Briefly summarize how adding this data point affects parts (a)-(g).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#ggplot(ggdat, aes(x=x, y=y))+}
\hlcom{#  geom_smooth(color="blue",}
\hlcom{#              method="lm",}
\hlcom{#              formula=y~x)+}
\hlcom{#  geom_point(shape=1,}
\hlcom{#             alpha=.3)+}
\hlcom{#  theme_bw()}
\hlcom{#there is one unusually small observation!}
\hlstd{four.model}\hlkwb{<-}\hlkwd{lm}\hlstd{(y}\hlopt{~}\hlstd{x,} \hlkwc{data}\hlstd{=ggdat)}
\hlkwd{plotResiduals}\hlstd{(four.model)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-28-1} 
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plotInfluence}\hlstd{(four.model)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-29-1} 
\end{knitrout}
    \item Add the resulting regression line to the plot in part (d) in blue.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#5.2+(2.06*x)}
\hlcom{#10.742+(1.891*x)}

\hlstd{ggdat.final} \hlkwb{<-} \hlstd{ggdat} \hlopt{%>%}
  \hlkwd{mutate}\hlstd{(}\hlkwc{beforeAdding} \hlstd{=} \hlnum{5.2}\hlopt{+}\hlstd{(}\hlnum{2.06}\hlopt{*}\hlstd{x),}
     \hlkwc{afterAdding} \hlstd{=} \hlnum{10.742}\hlopt{+}\hlstd{(}\hlnum{1.891}\hlopt{*}\hlstd{x))}\hlopt{%>%}
  \hlkwd{pivot_longer}\hlstd{(}\hlkwc{cols}\hlstd{=}\hlkwd{ends_with}\hlstd{(}\hlstr{"Adding"}\hlstd{),}
           \hlkwc{names_to}\hlstd{=}\hlstr{"Plot"}\hlstd{,}
           \hlkwc{values_to}\hlstd{=}\hlstr{"Pred"}\hlstd{)}


\hlkwd{ggplot}\hlstd{(ggdat.final,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=Pred))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{color}\hlstd{=Plot),}
          \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
          \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y}\hlstd{=y),} \hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
         \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{scale_color_manual}\hlstd{(}\hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"black"}\hlstd{,} \hlstr{"blue"}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-30-1} 
\end{knitrout}
    \item Refit this model using several robust techniques for dealing with the
    bad observation. Create a plot that summarizes all the approaches taken, and 
    use a metric to select the best model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(MASS)}
\hlkwd{library}\hlstd{(sfsmisc)}
\hlcom{#Hubert}
\hlcom{#6.0391 + (2.0421 * x)}
\hlstd{mod.hubert} \hlkwb{<-} \hlkwd{rlm}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{data}\hlstd{=ggdat,}
              \hlkwc{psi}\hlstd{=psi.huber)}
\hlkwd{summary}\hlstd{(mod.hubert)}
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, data = ggdat, psi = psi.huber)
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -185.2474   -3.1410    0.5295    3.2745   15.3087 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)  6.0391  1.4224     4.2457
## x            2.0421  0.0228    89.7473
## 
## Residual standard error: 4.747 on 49 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{f.robftest}\hlstd{(mod.hubert,} \hlkwc{var}\hlstd{=}\hlstr{"x"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## 	robust F-test (as if non-random weights)
## 
## data:  from rlm(formula = y ~ x, data = ggdat, psi = psi.huber)
## F = 7932.4, p-value < 2.2e-16
## alternative hypothesis: true x is not equal to 0
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#5.7016 + (2.0504 * x)}
\hlstd{mod.bisquare}\hlkwb{<-}\hlkwd{rlm}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{data}\hlstd{=ggdat,}
                  \hlkwc{psi}\hlstd{=psi.bisquare)}

\hlkwd{summary}\hlstd{(mod.bisquare)}
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, data = ggdat, psi = psi.bisquare)
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -185.739   -3.265    0.056    3.033   15.011 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)  5.7016  1.4342     3.9755
## x            2.0504  0.0229    89.3722
## 
## Residual standard error: 4.607 on 49 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{f.robftest}\hlstd{(mod.bisquare,} \hlkwc{var}\hlstd{=}\hlstr{"x"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## 	robust F-test (as if non-random weights)
## 
## data:  from rlm(formula = y ~ x, data = ggdat, psi = psi.bisquare)
## F = 7888.7, p-value < 2.2e-16
## alternative hypothesis: true x is not equal to 0
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(quantreg)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: SparseM}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# Attaching package: 'SparseM'}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following object is masked from 'package:base':\\\#\# \\\#\# \ \ \ \ backsolve}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in .recacheSubclasses(def@className, def, env): undefined subclass "{}numericVector"{} of class "{}Mnumeric"{}; definition not updated}}\begin{alltt}
\hlcom{#5.52 + (2.05*x)}
\hlstd{mod.quant} \hlkwb{<-} \hlkwd{rq}\hlstd{(y}\hlopt{~}\hlstd{x,} \hlkwc{data}\hlstd{=ggdat)}
\hlkwd{summary}\hlstd{(mod.quant,} \hlkwc{se} \hlstd{=} \hlstr{"ker"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call: rq(formula = y ~ x, data = ggdat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             Value    Std. Error t value  Pr(>|t|)
## (Intercept)  5.52886  2.88708    1.91504  0.06133
## x            2.05271  0.04751   43.20164  0.00000
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#6.0391 + (2.0421 * x) â Huber}
\hlcom{#5.7016 + (2.0504 * x) â Bisquare}
\hlcom{#5.52 + (2.05*x) â Quantile}

\hlstd{ggdat.compare} \hlkwb{<-} \hlstd{ggdat} \hlopt{%>%}
  \hlkwd{mutate}\hlstd{(}\hlkwc{original_reg} \hlstd{=} \hlnum{5.2}\hlopt{+}\hlstd{(}\hlnum{2.06}\hlopt{*}\hlstd{x),}
         \hlkwc{OLS_reg} \hlstd{=} \hlnum{10.742}\hlopt{+}\hlstd{(}\hlnum{1.891}\hlopt{*}\hlstd{x),}
         \hlkwc{Huber_reg} \hlstd{=} \hlnum{6.0391} \hlopt{+} \hlstd{(}\hlnum{2.0421} \hlopt{*} \hlstd{x),}
         \hlkwc{Bisquare_reg} \hlstd{=} \hlnum{5.7016} \hlopt{+} \hlstd{(}\hlnum{2.0504} \hlopt{*} \hlstd{x),}
         \hlkwc{Quantile_reg} \hlstd{=} \hlnum{5.52} \hlopt{+} \hlstd{(}\hlnum{2.05}\hlopt{*}\hlstd{x),}
         \hlkwc{Population_reg} \hlstd{=} \hlnum{3.5}\hlopt{+}\hlstd{(}\hlnum{2.1}\hlopt{*}\hlstd{x))}\hlopt{%>%}
  \hlkwd{pivot_longer}\hlstd{(}\hlkwc{cols}\hlstd{=}\hlkwd{ends_with}\hlstd{(}\hlstr{"_reg"}\hlstd{),}
               \hlkwc{names_to}\hlstd{=}\hlstr{"Plot"}\hlstd{,}
               \hlkwc{values_to}\hlstd{=}\hlstr{"Pred"}\hlstd{)}


\hlkwd{ggplot}\hlstd{(ggdat.compare,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=x,} \hlkwc{y}\hlstd{=Pred))}\hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{color}\hlstd{=Plot),}
              \hlkwc{method}\hlstd{=}\hlstr{"lm"}\hlstd{,}
              \hlkwc{formula}\hlstd{=y}\hlopt{~}\hlstd{x)}\hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y}\hlstd{=y),} \hlkwc{shape}\hlstd{=}\hlnum{1}\hlstd{,}
             \hlkwc{alpha}\hlstd{=}\hlnum{.3}\hlstd{)}\hlopt{+}
  \hlkwd{scale_color_manual}\hlstd{(}\hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"black"}\hlstd{,} \hlstr{"blue"}\hlstd{,} \hlstr{"green"}\hlstd{,} \hlstr{"red"}\hlstd{,} \hlstr{"yellow"}\hlstd{,} \hlstr{"purple"}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-34-1} 
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(Metrics)}

\hlcom{#rmse(ggdat$y, predict(mod.quant))}
\hlcom{#rmse(ggdat$y, predict(mod.hubert))}
\hlcom{#rmse(ggdat$y, predict(mod.bisquare))}


\hlcom{#rmse(test1, test2)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#test1}
\hlcom{#test2}
\hlcom{#rmse(ggdat$y, predict(mod.hubert))}
\end{alltt}
\end{kframe}
\end{knitrout}
  \end{enumerate}
  \end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End File
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\newpage
\bibliography{bib}
\end{document}
