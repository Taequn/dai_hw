ggplot(ggdat.final, aes(x=x, y=y))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(shape=1,
alpha=.3)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
ggdat.final <- ggdat %>%
mutate(beforeAdding = 5.2+(2.06*x),
afterAdding = 10.742+(1.891*x))%>%
pivot_longer(cols=ends_with("Adding"),
names_to="Plot",
values_to="Pred")
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
library(rcompanion)
install.packages("rcompanion")
library(rcompanion)
library(MASS)
??transformTukey
ggplot(ggdat, aes(x=x, y=y))+
geom_smooth(color="blue",
method="lm",
formula=y~x)+
geom_point(shape=1,
alpha=.3)+
theme_bw()
plotResiduals(ggdat)
#not stealing your function, prof. c:
plotResiduals<-function(mod){
ggdat <- data.frame(y.hat = fitted(mod),
e = residuals(mod))
########################################
####  Fitted versus residuals
########################################
p1<-ggplot(data=ggdat,aes(x=y.hat,y=e))+
geom_point(shape=1)+
geom_hline(yintercept=0,linetype="dashed")+
theme_bw()+
xlab(bquote(hat(Y)))+
ylab("Residuals")+
ggtitle("Fitted Values versus the Residuals")
########################################
####  Histogram with Gaussian Density
########################################
ggdat.gaussian<-data.frame(x=seq(min(ggdat$e)-sd(ggdat$e),
max(ggdat$e)+sd(ggdat$e),length.out = 500),
f=dnorm(seq(min(ggdat$e)-sd(ggdat$e),
max(ggdat$e)+sd(ggdat$e),length.out = 500),
#ei should have mean zero
mean=0,
#ei should have common variance
sd=summary(mod)$sigma),
CDF=pnorm(seq(min(ggdat$e)-sd(ggdat$e),
max(ggdat$e)+sd(ggdat$e),length.out = 500),
#ei should have mean zero
mean=0,
#ei should have common variance
sd=summary(mod)$sigma))
d<-density(ggdat$e)
p2<-ggplot(data=ggdat,aes(x=e))+
geom_histogram(aes(y=..density..), binwidth = d$bw,
fill="lightblue",color="black")+
geom_density(aes(color="Empirical"),size=1,
trim=F,show.legend = F)+
geom_line(data=ggdat.gaussian,aes(x=x,y=f,color="Gaussian-Assumed"),
size=1)+
theme_bw()+
xlab("Residual")+
ylab("Density")+
labs(color = "")+
theme(legend.position="bottom")+
ggtitle("Residual Density")
########################################
####  Plot the residuals ecdf
########################################
e.cdf.func<-ecdf(ggdat$e)
e.cdf<-e.cdf.func(sort(ggdat$e))
ggdat<-ggdat %>% mutate(e.sort=sort(ggdat$e),
e.cdf=e.cdf)
p3<-ggplot(data=ggdat,aes(x=e.sort))+
geom_line(data=ggdat.gaussian,aes(x=x,y=CDF,color="Gaussian-Assumed"),size=1)+
geom_step(aes(y=e.cdf,color="Empirical"),show.legend = F,size=1)+
geom_hline(yintercept=0)+
theme_bw()+
xlab("Residual")+
ylab("Cumulative Density")+
labs(color = "")+
theme(legend.position="bottom")+
ggtitle("Residual Cumulative Density")
########################################
####  QQplot of the residuals
########################################
library("qqplotr")
p4<-ggplot(data=ggdat,aes(sample=scale(e)))+ #standardize e
stat_qq_band(alpha=0.25) +
stat_qq_line() +
stat_qq_point() +
theme_bw()+
xlab("Gaussian Quantiles")+
ylab("Sample Quantiles")+
ggtitle("Normal Quantile-Quantile Plot of Residuals")
########################################
####  Print
########################################
(p1|p4)/(p2|p3)
}
library(patchwork)
plotResiduals(ggdat)
ggdat <- rbind(ggdat,     # original data
c(100,25)) # bad observation
plotResiduals(ggdat)
#there is one unusually small observation!
four.model<-lm(y~x, data=ggdat)
plotResiduals(four.model)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(group=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
four.model.r <- rlm(y~x, data=ggdat)
summary(four.model.r)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=c("Blue, Black")),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)
library(tidyverse)
set.seed(7272)
n<-50
ggdat <- data.frame(x=sample(x=seq(0,100,0.01),size=n,replace=TRUE)) %>%
mutate(y=3.5+2.1*x+rnorm(n=n,mean=0,sd=5))
four.model<-lm(y~x, data=ggdat)
answer3<-summary(four.model)
answer3$coefficients
ggplot(ggdat, aes(x=x, y=y))+
geom_smooth(color="black",
method="lm",
formula=y~x)+
geom_point(shape=1,
alpha=.3)+
theme_bw()+
labs(title="Fitting the linear regression model",
subtitle="For simulated data")
source("https://cipolli.com/students/code/plotResiduals.R")
source("https://cipolli.com/students/code/plotInfluence.R")
ggdat.compare <- ggdat %>%
mutate(original_reg = 5.2+(2.06*x),
OLS_reg = 10.742+(1.891*x),
Huber_reg = 6.0391 + (2.0421 * x),
Bisquare_reg = 5.7016 + (2.0504 * x),
Quantile_reg = 5.52 + (2.05*x),
Population_reg = 3.5+(2.1*x))%>%
pivot_longer(cols=ends_with("_reg"),
names_to="Plot",
values_to="Pred")
ggplot(ggdat.compare, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)+
scale_color_manual(values=c("black", "blue", "green", "red", "yellow", "purple"),
labels=c("Bisquare", "Huber", "OLS", "Before outlier", "Population",
"Quantile"))+
labs(title="Fitting the linear regression models",
subtitle="For simulated data with an outlier",
color="Technique")
ggdat <- rbind(ggdat,     # original data
c(100,25)) # bad observation
four.model<-lm(y~x, data=ggdat)
#5.2+(2.06*x)
#10.742+(1.891*x)
ggdat.final <- ggdat %>%
mutate(beforeAdding = 5.2+(2.06*x),
afterAdding = 10.742+(1.891*x))%>%
pivot_longer(cols=ends_with("Adding"),
names_to="Plot",
values_to="Pred")
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)+
scale_color_manual(values=c("black", "blue"),
label=c("After",
"Before"))+
labs(title="Fitting the linear regression models",
subtitle="For simulated data with an outlier",
color="Adding an outlier")
library(MASS)
library(sfsmisc)
#Hubert
#6.0391 + (2.0421 * x) + e
mod.hubert <- rlm(y ~ x, data=ggdat,
psi=psi.huber)
summary(mod.hubert)
ggdat.compare <- ggdat %>%
mutate(original_reg = 5.2+(2.06*x),
OLS_reg = 10.742+(1.891*x),
Huber_reg = 6.0391 + (2.0421 * x),
Bisquare_reg = 5.7016 + (2.0504 * x),
Quantile_reg = 5.52 + (2.05*x),
Population_reg = 3.5+(2.1*x))%>%
pivot_longer(cols=ends_with("_reg"),
names_to="Plot",
values_to="Pred")
ggplot(ggdat.compare, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)+
scale_color_manual(values=c("black", "blue", "green", "red", "yellow", "purple"),
labels=c("Bisquare", "Huber", "OLS", "Before outlier", "Population",
"Quantile"))+
labs(title="Fitting the linear regression models",
subtitle="For simulated data with an outlier",
color="Technique")
library(Metrics)
?rmse
rmse(ggdat$y, predict(mod.quant))
#5.52 + (2.05*x)
mod.quant <- rq(y~x, data=ggdat)
#5.7016 + (2.0504 * x)
mod.bisquare<-rlm(y ~ x, data=ggdat,
psi=psi.bisquare)
#Hubert
#6.0391 + (2.0421 * x) + e
mod.hubert <- rlm(y ~ x, data=ggdat,
psi=psi.huber)
rmse(ggdat$y, predict(mod.quant))
#5.52 + (2.05*x)
mod.quant <- rq(y~x, data=ggdat)
library(quantreg)
#5.52 + (2.05*x)
mod.quant <- rq(y~x, data=ggdat)
rmse(ggdat$y, predict(mod.quant))
rmse(ggdat$y, predict(mod.hubert))
rmse(ggdat$y, predict(mod.bisquare))
#Body of the function
aovFunc <- function(rand.n=FALSE, rand.s=FALSE, equal.m=TRUE, n=5, loop=0,
welch=FALSE){
alpha<-0.05
count=0
#if random number of n is true, we'd use random number
#from uniform distribution
if(rand.n){
n1=round(runif(1, min=5, max=100),0)
n2=round(runif(1, min=5, max=100),0)
n3=round(runif(1, min=5, max=100),0)
n4=round(runif(1, min=5, max=100),0)
}
#else: use the number passed on within the function (default: 5)
else{
n1=n
n2=n
n3=n
n4=n
}
if(rand.s){
s1=rgamma(1, 2, 1)
s2=rgamma(1, 2, 1)
s3=rgamma(1, 2, 1)
s4=rgamma(1, 2, 1)
}else{
s1=2
s2=2
s3=2
s4=2
}
for(i in 1:loop){
if(equal.m){
#if mean is equal, pass 0 to the function
t1<-rlaplace(n=n1, m=0, s=s1)
label1<-"T1"
t2<-rlaplace(n=n2, m=0, s=s2)
label2<-"T2"
t3<-rlaplace(n=n3, m=0, s=s3)
label3<-"T3"
t4<-rlaplace(n=n4, m=0, s=s4)
label4<-"T4"
}
else{
#else: pass 1 to the last function
t1<-rlaplace(n=n1, m=0, s=s1)
label1<-"T1"
t2<-rlaplace(n=n2, m=0, s=s2)
label2<-"T2"
t3<-rlaplace(n=n3, m=0, s=s3)
label3<-"T3"
t4<-rlaplace(n=n4, m=1, s=s4)
label4<-"T4"
}
#passing the data with labels
dat<-data.frame(value=c(t1, t2, t3, t4),
group=c(rep(c("T1", "T2", "T3", "T4"),
times=c(length(t1),
length(t2),
length(t3),
length(t4)))))
#if we're not using Welch's ANOVA, go here
if(!welch){
anova<-summary(aov(value~group, data=dat))
sum_test <- unlist((anova))
p.value<-sum_test["Pr(>F)1"]
#print(p.value) #bugtest
}
#Use Welch's ANOVA to assess its results when S is different
#NOT USED BUT STILL PRETTY COOL
else{
anova_w<-welch_anova_test(value~group, data=dat)
p.value<-anova_w$p
}
if(p.value<0.05){
count=count+1
}
}
count/loop
}
aovFunc(equal.m=FALSE, loop=1000, n=50)
library(rstatix)
#Body of the function
aovFunc <- function(rand.n=FALSE, rand.s=FALSE, equal.m=TRUE, n=5, loop=0,
welch=FALSE){
alpha<-0.05
count=0
#if random number of n is true, we'd use random number
#from uniform distribution
if(rand.n){
n1=round(runif(1, min=5, max=100),0)
n2=round(runif(1, min=5, max=100),0)
n3=round(runif(1, min=5, max=100),0)
n4=round(runif(1, min=5, max=100),0)
}
#else: use the number passed on within the function (default: 5)
else{
n1=n
n2=n
n3=n
n4=n
}
if(rand.s){
s1=rgamma(1, 2, 1)
s2=rgamma(1, 2, 1)
s3=rgamma(1, 2, 1)
s4=rgamma(1, 2, 1)
}else{
s1=2
s2=2
s3=2
s4=2
}
for(i in 1:loop){
if(equal.m){
#if mean is equal, pass 0 to the function
t1<-rlaplace(n=n1, m=0, s=s1)
label1<-"T1"
t2<-rlaplace(n=n2, m=0, s=s2)
label2<-"T2"
t3<-rlaplace(n=n3, m=0, s=s3)
label3<-"T3"
t4<-rlaplace(n=n4, m=0, s=s4)
label4<-"T4"
}
else{
#else: pass 1 to the last function
t1<-rlaplace(n=n1, m=0, s=s1)
label1<-"T1"
t2<-rlaplace(n=n2, m=0, s=s2)
label2<-"T2"
t3<-rlaplace(n=n3, m=0, s=s3)
label3<-"T3"
t4<-rlaplace(n=n4, m=1, s=s4)
label4<-"T4"
}
#passing the data with labels
dat<-data.frame(value=c(t1, t2, t3, t4),
group=c(rep(c("T1", "T2", "T3", "T4"),
times=c(length(t1),
length(t2),
length(t3),
length(t4)))))
#if we're not using Welch's ANOVA, go here
if(!welch){
anova<-summary(aov(value~group, data=dat))
sum_test <- unlist((anova))
p.value<-sum_test["Pr(>F)1"]
#print(p.value) #bugtest
}
#Use Welch's ANOVA to assess its results when S is different
#NOT USED BUT STILL PRETTY COOL
else{
anova_w<-welch_anova_test(value~group, data=dat)
p.value<-anova_w$p
}
if(p.value<0.05){
count=count+1
}
}
count/loop
}
aovFunc(equal.m=FALSE, loop=1000, n=50)
library(rmutil)
library(tidyverse)
aovFunc(equal.m=FALSE, loop=1000, n=50)
\item Repeat the simulation study in (b-c), except with $n=50$.
Comment on the results of this simulation.
<<warning=FALSE,alert=FALSE, message=FALSE>>=
aovFunc(equal.m=TRUE, loop=1000, n=50)
aovFunc(equal.m=FALSE, loop=1000, n=50)
@
With a sample size as big as 50, on average, ANOVA is able to reject 95-96 percent of hypothesis when the means are not equal. It implies that ANOVA is highly sensative to the sample size, and it's imperative for researchers to maintain at least $n>30$!
aovFunc(equal.m=FALSE, loop=1000, n=50)
aovFunc(equal.m=FALSE, rand.n=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, loop=1000)
aovFunc(equal.m=FALSE, loop=1000, n=50)
aovFunc(equal.m=FALSE, loop=1000, n=50)
aovFunc(equal.m=FALSE, loop=1000, n=50)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
aovFunc(equal.m=FALSE, rand.n=TRUE, rand.s=TRUE, loop=1000)
library(tidyverse)
set.seed(7272)
n<-50
ggdat <- data.frame(x=sample(x=seq(0,100,0.01),size=n,replace=TRUE)) %>%
mutate(y=3.5+2.1*x+rnorm(n=n,mean=0,sd=5))
four.model<-lm(y~x, data=ggdat)
answer3<-summary(four.model)
answer3$coefficients
ggplot(ggdat, aes(x=x, y=y))+
geom_smooth(color="black",
method="lm",
formula=y~x)+
geom_point(shape=1,
alpha=.3)+
theme_bw()+
labs(title="Fitting the linear regression model",
subtitle="For simulated data")
ggdat <- rbind(ggdat,     # original data
c(100,25)) # bad observation
four.model<-lm(y~x, data=ggdat)
#5.2+(2.06*x)
#10.742+(1.891*x)
ggdat.final <- ggdat %>%
mutate(beforeAdding = 5.2+(2.06*x),
afterAdding = 10.742+(1.891*x))%>%
pivot_longer(cols=ends_with("Adding"),
names_to="Plot",
values_to="Pred")
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)+
scale_color_manual(values=c("black", "blue"),
label=c("After",
"Before"))+
labs(title="Fitting the linear regression models",
subtitle="For simulated data with an outlier",
color="Adding an outlier")
ggplot(ggdat.final, aes(x=x, y=Pred))+
geom_smooth(aes(color=Plot),
method="lm",
formula=y~x)+
geom_point(aes(y=y), shape=1,
alpha=.3)+
scale_color_manual(values=c("black", "blue"),
label=c("After",
"Before"))+
labs(title="Fitting the linear regression models",
subtitle="For simulated data with an outlier",
color="Adding an outlier",
y="y")
